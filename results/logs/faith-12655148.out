================================================================================
JobID = 12655148
User = u25472, Account = ag_gipp
Partition = scc-gpu, Nodelist = ggpu195
================================================================================
Starting vLLM server (model=Qwen/Qwen3-32B) -> results/logs/vllm_annotate_12655148.log
vLLM ready

Running annotator with model=Qwen/Qwen3-32B port=8000
[INFO] Initializing vLLM client at http://127.0.0.1:8000/v1
  0%|          | 0/77 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s]
  0%|          | 0/77 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/mnt/vast-standard/home/lucajoshua.francis/u25472/fact-llm/src/faith_shop.py", line 197, in <module>
    baseline_reasoning = model.invoke(messages)
                         ^^^^^^^^^^^^^^^^^^^^^^
  File "/user/lucajoshua.francis/u25472/.conda/envs/fact-llm-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 398, in invoke
    self.generate_prompt(
  File "/user/lucajoshua.francis/u25472/.conda/envs/fact-llm-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 1117, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/user/lucajoshua.francis/u25472/.conda/envs/fact-llm-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 927, in generate
    self._generate_with_cache(
  File "/user/lucajoshua.francis/u25472/.conda/envs/fact-llm-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 1221, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/user/lucajoshua.francis/u25472/.conda/envs/fact-llm-env/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 1382, in _generate
    raise e
  File "/user/lucajoshua.francis/u25472/.conda/envs/fact-llm-env/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 1377, in _generate
    raw_response = self.client.with_raw_response.create(**payload)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/user/lucajoshua.francis/u25472/.conda/envs/fact-llm-env/lib/python3.11/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "/user/lucajoshua.francis/u25472/.conda/envs/fact-llm-env/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/user/lucajoshua.francis/u25472/.conda/envs/fact-llm-env/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/user/lucajoshua.francis/u25472/.conda/envs/fact-llm-env/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/user/lucajoshua.francis/u25472/.conda/envs/fact-llm-env/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `Qwen/Qwen3-32B` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}
============ Job Information ===================================================
Submitted: 2026-02-15T18:56:00
Started: 2026-02-15T18:57:00
Ended: 2026-02-15T18:57:09
Elapsed: 1 min, Limit: 60 min, Difference: 59 min
CPUs: 8, Nodes: 1
Estimated Consumption: 2.50 core-hours
================================================================================
