[0;36m(APIServer pid=391258)[0;0m INFO 01-15 14:48:52 [api_server.py:1351] vLLM API server version 0.13.0
[0;36m(APIServer pid=391258)[0;0m INFO 01-15 14:48:52 [utils.py:253] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'enable_auto_tool_choice': True, 'tool_call_parser': 'hermes', 'model': 'Qwen/Qwen3-32B', 'trust_remote_code': True, 'max_model_len': 40960, 'download_dir': '/scratch-scc/projects/ag_gipp/u25472/huggingface'}
[0;36m(APIServer pid=391258)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=391258)[0;0m INFO 01-15 14:48:53 [model.py:514] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=391258)[0;0m INFO 01-15 14:48:53 [model.py:1661] Using max model len 40960
[0;36m(APIServer pid=391258)[0;0m INFO 01-15 14:48:54 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 14:49:15 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='Qwen/Qwen3-32B', speculative_config=None, tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40960, download_dir='/scratch-scc/projects/ag_gipp/u25472/huggingface', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 14:49:16 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.241.148.49:37575 backend=nccl
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 14:49:16 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 14:49:18 [gpu_model_runner.py:3562] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 14:49:20 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 14:58:05 [weight_utils.py:487] Time spent downloading weights for Qwen/Qwen3-32B: 524.355602 seconds
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:10<02:55, 10.94s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:21<02:38, 10.54s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:31<02:27, 10.55s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:42<02:18, 10.63s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:52<02:05, 10.48s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  35% Completed | 6/17 [01:02<01:54, 10.37s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  41% Completed | 7/17 [01:13<01:43, 10.31s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  47% Completed | 8/17 [01:23<01:33, 10.37s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  53% Completed | 9/17 [01:34<01:23, 10.42s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  59% Completed | 10/17 [01:48<01:20, 11.56s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  65% Completed | 11/17 [02:00<01:10, 11.82s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  71% Completed | 12/17 [02:09<00:53, 10.78s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  76% Completed | 13/17 [02:19<00:42, 10.68s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  82% Completed | 14/17 [02:29<00:31, 10.54s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  88% Completed | 15/17 [02:40<00:20, 10.48s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards:  94% Completed | 16/17 [02:50<00:10, 10.45s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards: 100% Completed | 17/17 [03:01<00:00, 10.51s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m Loading safetensors checkpoint shards: 100% Completed | 17/17 [03:01<00:00, 10.65s/it]
[0;36m(EngineCore_DP0 pid=401077)[0;0m 
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 15:01:07 [default_loader.py:308] Loading weights took 181.15 seconds
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 15:01:07 [gpu_model_runner.py:3659] Model loading took 61.0347 GiB memory and 708.485594 seconds
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 15:01:19 [backends.py:643] Using cache directory: /user/lucajoshua.francis/u25472/.cache/vllm/torch_compile_cache/0bf9477d19/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 15:01:19 [backends.py:703] Dynamo bytecode transform time: 12.05 s
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 15:01:35 [backends.py:261] Cache the graph of compile range (1, 2048) for later use
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 15:01:50 [backends.py:278] Compiling a graph for compile range (1, 2048) takes 21.10 s
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 15:01:50 [monitor.py:34] torch.compile takes 33.14 s in total
[0;36m(EngineCore_DP0 pid=401077)[0;0m INFO 01-15 15:01:52 [gpu_worker.py:375] Available KV cache memory: 8.76 GiB
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=401077)[0;0m ERROR 01-15 15:01:53 [core.py:866] ValueError: To serve at least one request with the models's max seq len (40960), (10.00 GiB KV cache is needed, which is larger than the available KV cache memory (8.76 GiB). Based on the available memory, the estimated maximum model length is 35888. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[0;36m(EngineCore_DP0 pid=401077)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=401077)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=401077)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=401077)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=401077)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=401077)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=401077)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=401077)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=401077)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=401077)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=401077)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=401077)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=401077)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=401077)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=401077)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=401077)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=401077)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=401077)[0;0m     kv_cache_configs = get_kv_cache_configs(
[0;36m(EngineCore_DP0 pid=401077)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=401077)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=401077)[0;0m     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=401077)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py", line 710, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=401077)[0;0m     raise ValueError(
[0;36m(EngineCore_DP0 pid=401077)[0;0m ValueError: To serve at least one request with the models's max seq len (40960), (10.00 GiB KV cache is needed, which is larger than the available KV cache memory (8.76 GiB). Based on the available memory, the estimated maximum model length is 35888. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W115 15:01:54.740117369 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=391258)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/bin/vllm", line 10, in <module>
[0;36m(APIServer pid=391258)[0;0m     sys.exit(main())
[0;36m(APIServer pid=391258)[0;0m              ^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=391258)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[0;36m(APIServer pid=391258)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 96, in run
[0;36m(APIServer pid=391258)[0;0m     return __asyncio.run(
[0;36m(APIServer pid=391258)[0;0m            ^^^^^^^^^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 195, in run
[0;36m(APIServer pid=391258)[0;0m     return runner.run(main)
[0;36m(APIServer pid=391258)[0;0m            ^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
[0;36m(APIServer pid=391258)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=391258)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=391258)[0;0m     return await main
[0;36m(APIServer pid=391258)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 1398, in run_server
[0;36m(APIServer pid=391258)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 1417, in run_server_worker
[0;36m(APIServer pid=391258)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=391258)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[0;36m(APIServer pid=391258)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=391258)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 172, in build_async_engine_client
[0;36m(APIServer pid=391258)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=391258)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[0;36m(APIServer pid=391258)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=391258)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 213, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=391258)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=391258)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 215, in from_vllm_config
[0;36m(APIServer pid=391258)[0;0m     return cls(
[0;36m(APIServer pid=391258)[0;0m            ^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 134, in __init__
[0;36m(APIServer pid=391258)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=391258)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
[0;36m(APIServer pid=391258)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=391258)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 820, in __init__
[0;36m(APIServer pid=391258)[0;0m     super().__init__(
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 477, in __init__
[0;36m(APIServer pid=391258)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=391258)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=391258)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=391258)[0;0m     next(self.gen)
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 903, in launch_core_engines
[0;36m(APIServer pid=391258)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=391258)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
[0;36m(APIServer pid=391258)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=391258)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
